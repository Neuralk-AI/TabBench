{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9dc27f-0ae2-4b43-935c-e175fcf144ed",
   "metadata": {},
   "source": [
    "**What is TabBench?**\n",
    "*TabBench* is a benchmark suite for tabular data focused on real-world business use cases like product categorization, deduplication, and pricing. Unlike academic benchmarks, it evaluates models on industrial datasets from sectors such as retail, banking, and insurance. Built on top of [Neuralk Foundry-CE](https://github.com/Neuralk-AI/NeuralkFoundry-CE), TabBench structures each task as a modular workflow, making it easy to test and compare different approaches. It’s designed to help identify the best models for practical, industry-driven challenges.\n",
    "\n",
    "---\n",
    "\n",
    "## Tackling the Categorization Challenge\n",
    "\n",
    "One of the core tasks in TabBench is **product categorization**: a classification problem that arises frequently in e-commerce and digital inventory management.\n",
    "\n",
    "Due to privacy restrictions, we are unable to share the 36 real-world product categorization datasets used internally. However, in this notebook, we provide a public dataset that mimics some of the challenges encountered in our industrial workloads. This allows you to experiment under realistic conditions and test the robustness of your models on comparable data.\n",
    "\n",
    "\n",
    "## Understanding Product Categorization\n",
    "\n",
    "At its core, product categorization is a **multi-class classification** problem. But unlike standard benchmarks, it comes with two key complications:\n",
    "\n",
    "1. **Heterogeneous and partially structured inputs:**\n",
    "   Product data is typically semi-structured. While all products may share fields like `title`, `description`, and `price`, other features — such as `battery power`, `screen size`, or `material` — may only apply to certain categories. For example, \"voltage\" is relevant for an electric drill but meaningless for a T-shirt. This inconsistency places an additional burden on preprocessing and feature engineering.\n",
    "\n",
    "   In our internal workflows, this preprocessing is handled with care. However, for privacy reasons, the dataset we share here includes only basic fields (`title` and `description`), which we embed using a pre-trained language model followed by dimensionality reduction.\n",
    "\n",
    "2. **Practical evaluation criteria:**\n",
    "   Many academic benchmarks rely on ROC-AUC, which is threshold-independent and useful for model calibration analysis. However, in production systems, such as those used in retail, the priority is often on **accuracy**, **F1-score**, or **precision/recall**, depending on the downstream impact of misclassifications.\n",
    "\n",
    "   Inspired by challenges like the [Rakuten Product Classification Challenge](https://challengedata.ens.fr/challenges/35), we use **F1-score** as the default metric in TabBench for this task. This choice better reflects industrial priorities, where both false positives and false negatives can be costly.\n",
    "\n",
    "## Preprocessing Pipeline\n",
    "\n",
    "To transform text into usable features, we use the [`TextEncoder`](https://skrub-data.org/stable/reference/generated/skrub.TextEncoder.html) class from skrub.\n",
    "\n",
    "This encoder uses the [`intfloat/multilingual-e5-base`](https://huggingface.co/intfloat/multilingual-e5-base) model from Hugging Face to generate dense embeddings for the `title` and `description` fields. Each of these results in a 768-dimensional vector.\n",
    "\n",
    "To reduce dimensionality and improve efficiency, we then apply **Principal Component Analysis (PCA)**, projecting each embedding down to 50 dimensions. The reduced embeddings for the title and description are concatenated into a single 100-dimensional feature vector per product.\n",
    "\n",
    "This minimal preprocessing pipeline is designed to be easy to reproduce and compatible with a wide range of models.\n",
    "\n",
    "## The Task at Hand\n",
    "\n",
    "Since the real product datasets used in TabBench are confidential, we provide a substitute based on the public **BestBuy** dataset. This dataset, introduced in [Notebook 1](./1%20-%20Getting%20Started%20with%20TabBench.ipynb), contains:\n",
    "\n",
    "* Product names (`title`)\n",
    "* Descriptions\n",
    "* A hierarchical category structure (level 1, 2, 3)\n",
    "\n",
    "This structure allows us to simulate real-world categorization scenarios, with multiple levels of granularity and class imbalance.\n",
    "\n",
    "Let’s begin by downloading the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "addd2562-ffd9-4c18-9ca8-ba7b61e59981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON from URL\n",
    "ds_url = 'https://raw.githubusercontent.com/BestBuyAPIs/open-data-set/refs/heads/master/products.json'\n",
    "response = requests.get(ds_url)\n",
    "data = response.json()  # this is a list of dicts\n",
    "\n",
    "# Extract relevant fields\n",
    "records = []\n",
    "for item in data:\n",
    "    record = {\n",
    "        \"id\": item.get(\"sku\"),\n",
    "        \"name\": item.get(\"name\"),\n",
    "        \"description\": item.get(\"description\"),\n",
    "        'type': item.get(\"type\"),\n",
    "        'price': item.get(\"price\"),\n",
    "        'manufacturer': item.get(\"manufacturer\"),\n",
    "    }\n",
    "    categories = item.get(\"category\", [])\n",
    "    for i in range(4):  # adjust to desired number of category levels\n",
    "        record[f\"category_{i+1}\"] = categories[i][\"name\"] if i < len(categories) else None\n",
    "    records.append(record)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827da192-baf1-4afa-be1a-0cf9885300e8",
   "metadata": {},
   "source": [
    "## Data Filtering\n",
    "\n",
    "To simplify this example, we remove *rare classes*, that is, categories with very few samples. This serves two purposes:\n",
    "\n",
    "1. **Avoiding class imbalance issues**:\n",
    "   In classification tasks, classes with very few examples can introduce noise, make training unstable, and skew evaluation metrics. While handling imbalanced data is important in production settings, it is beyond the scope of this tutorial.\n",
    "\n",
    "2. **Speeding up experimentation**:\n",
    "   By filtering out underrepresented classes, we reduce the dataset size, which shortens both preprocessing and training time. This allows for quicker iteration during model development.\n",
    "\n",
    "In practice, handling rare classes might involve techniques such as class reweighting, oversampling, or hierarchical classification — all of which can be explored once the baseline is established.\n",
    "\n",
    "Let’s proceed with filtering out classes that appear too infrequently in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa86edc-65f3-42d0-a9e5-8c13c91d47d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top categories ['Connected Home & Housewares' 'Car Electronics & GPS'\n",
      " 'Computers & Tablets' 'Appliances' 'Audio' 'Cameras & Camcorders'\n",
      " 'Cell Phones']\n"
     ]
    }
   ],
   "source": [
    "# We only keep categories with more than 1000 products\n",
    "\n",
    "def filter_by_group(df, group, count):\n",
    "    counts = df.groupby(group)[group].transform('count')\n",
    "    return df[counts >= count]\n",
    "\n",
    "df = filter_by_group(df, 'category_3', 100)\n",
    "df = filter_by_group(df, 'category_2', 500)\n",
    "df = filter_by_group(df, 'category_1', 1000)\n",
    "\n",
    "print('Top categories', df['category_1'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0977175a-42b5-4324-a262-51f3ea130455",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Since we will be running multiple workflows on the same dataset, we apply the preprocessing step **once on the entire dataset**. This ensures consistency across all experiments and avoids redundant computation.\n",
    "\n",
    "In this example, we focus on transforming the text-based fields (`title` and `description`), which are common in product catalogs. These fields are first encoded using the [`intfloat/multilingual-e5-base`](https://huggingface.co/intfloat/multilingual-e5-base) model, a pre-trained transformer from Hugging Face optimized for multilingual sentence embeddings.\n",
    "\n",
    "Each text field is transformed into a dense vector of 768 floating-point values. To make these embeddings more manageable and reduce computation downstream, we apply **Principal Component Analysis (PCA)** and retain the top 50 components per field. The compressed vectors for `title` and `description` are then concatenated, yielding a final feature vector of 100 dimensions per product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3beaaf4b-3a79-40f5-977c-56dc6715d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TextEncoder\n",
    "\n",
    "text_encoder = TextEncoder(model_name='intfloat/multilingual-e5-base', n_components=50)\n",
    "\n",
    "name_encoded = text_encoder.fit_transform(df['name'])\n",
    "desc_encoded = text_encoder.fit_transform(df['description'])\n",
    "\n",
    "# Concatenate all\n",
    "X = pd.concat([name_encoded, desc_encoded], axis=1)\n",
    "X.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed64a610-b1a6-47eb-a486-396b98faa8b2",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Now that the data is preprocessed, we move on to the classification step.\n",
    "\n",
    "The BestBuy dataset includes a **hierarchical category structure** with three levels: `level_1`, `level_2`, and `level_3`. To simplify the experimental setup and focus on meaningful comparisons, we define one classification task per level. For each level:\n",
    "\n",
    "* We **select the subset of data corresponding to the most populated parent category** from the level above.\n",
    "  This ensures that the classification problem remains well-posed and avoids severe class imbalance from sparsely populated branches in the hierarchy.\n",
    "\n",
    "The resulting classification tasks are:\n",
    "\n",
    "| Level   | Samples | Classes |\n",
    "| ------- | ------- | ------- |\n",
    "| Level 1 | 23,777  | 7       |\n",
    "| Level 2 | 6,327   | 4       |\n",
    "| Level 3 | 5,891   | 7       |\n",
    "\n",
    "For each task, we train and evaluate a classifier using **5-fold cross-validation** and report the **average score** across folds. This approach gives a more reliable estimate of generalization performance than a single train-test split.\n",
    "\n",
    "> ☕ **Heads up:** This step may take a few minutes depending on your hardware and the model used. It might be a good time to grab a coffee while the models train!\n",
    "\n",
    "Once complete, we’ll visualize the results to compare performance across category levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd43ab15-63b0-4019-b764-def6e2cabbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 3\n",
      "-------\n",
      "X: 5891 rows × 100 features\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iPhone Accessories</td>\n",
       "      <td>2079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cell Phone Cases &amp; Clips</td>\n",
       "      <td>1847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cell Phone Batteries &amp; Power</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adapters, Cables &amp; Chargers</td>\n",
       "      <td>569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Smartwatches &amp; Accessories</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Screen Protectors</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Photography Accessories</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Label  Count\n",
       "0            iPhone Accessories   2079\n",
       "1      Cell Phone Cases & Clips   1847\n",
       "2  Cell Phone Batteries & Power    801\n",
       "3   Adapters, Cables & Chargers    569\n",
       "4    Smartwatches & Accessories    358\n",
       "5             Screen Protectors    123\n",
       "6       Photography Accessories    114"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 3: F1 = 0.944 ± 0.015\n",
      "Level 2\n",
      "-------\n",
      "X: 6327 rows × 100 features\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Small Kitchen Appliances</td>\n",
       "      <td>3342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ranges, Cooktops &amp; Ovens</td>\n",
       "      <td>1331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heating, Cooling &amp; Air Quality</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Refrigerators</td>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Label  Count\n",
       "0        Small Kitchen Appliances   3342\n",
       "1        Ranges, Cooktops & Ovens   1331\n",
       "2  Heating, Cooling & Air Quality    965\n",
       "3                   Refrigerators    689"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 2: F1 = 0.985 ± 0.003\n",
      "Level 1\n",
      "-------\n",
      "X: 23777 rows × 100 features\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Appliances</td>\n",
       "      <td>6327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cell Phones</td>\n",
       "      <td>5891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Computers &amp; Tablets</td>\n",
       "      <td>3614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Audio</td>\n",
       "      <td>2134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Connected Home &amp; Housewares</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Car Electronics &amp; GPS</td>\n",
       "      <td>1948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cameras &amp; Camcorders</td>\n",
       "      <td>1876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Label  Count\n",
       "0                   Appliances   6327\n",
       "1                  Cell Phones   5891\n",
       "2          Computers & Tablets   3614\n",
       "3                        Audio   2134\n",
       "4  Connected Home & Housewares   1987\n",
       "5        Car Electronics & GPS   1948\n",
       "6         Cameras & Camcorders   1876"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1: F1 = 0.973 ± 0.004\n"
     ]
    }
   ],
   "source": [
    "from neuralk_foundry_ce.workflow.use_cases import Categorisation\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"error\", category=UserWarning) \n",
    "\n",
    "# Simulate 3 levels of problems\n",
    "\n",
    "for level in [3, 2, 1]:\n",
    "    print(f'Level {level}')\n",
    "    print('-------')\n",
    "    # Keep samples from the most populated category at level - 1\n",
    "    X_level = X\n",
    "    y_level = df[f'category_{level}']\n",
    "    if level > 1:\n",
    "        top_n_1_value = df[f'category_{level - 1}'].value_counts().idxmax()\n",
    "        mask = (df[f'category_{level - 1}'] == top_n_1_value).values\n",
    "        X_level = X_level[mask]\n",
    "        y_level = y_level[mask]\n",
    "    X_level = X_level.reset_index(drop=True)\n",
    "    y_level = y_level.values\n",
    "\n",
    "    labels, counts = np.unique(y_level, return_counts=True)\n",
    "    n_samples, n_features = X_level.shape\n",
    "    print(f\"X: {n_samples} rows × {n_features} features\")\n",
    "    \n",
    "    # Tabular display of class distribution\n",
    "    df_counts = pd.DataFrame({\"Label\": labels, \"Count\": counts\n",
    "    }).sort_values(\"Count\", ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    display(df_counts)\n",
    "        \n",
    "    workflow = Categorisation()\n",
    "    f1_scores = []\n",
    "    for fold_index in range(5):\n",
    "        data, metrics = workflow.run(X_level, y_level, fold_index=fold_index)\n",
    "        f1_scores.append(metrics['xgboost-classifier']['test_f1_score'])\n",
    "\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "    print(f\"Level {level}: F1 = {mean_f1:.3f} ± {std_f1:.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a95111f-4ecf-4958-857e-0f37d56e881e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
